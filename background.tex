\section{Deep Learning Background}
We identify three main reasons why deep learning succeed in many areas related
to artificial intelligence, as well as their implications on network intrusion detection problem.

\subsection{Learning/Training Techniques}
In the supervise learning framework, given the feature representations and inference model,
learning is in essential an optimization problem which minimizes a predefined loss function
over given training examples.
The most commonly used optimization algorithm is back-propagation(BP)~\cite{Backpropagation} with gradient descent,
because computing gradient is Hessian-free and memoization saves a great amount of computation when
propagating backward level by level.
However, it is impossible to train deep neural networks and achieve optimal parameters only with BP.
The first problem is that usually the cost function is non-convex with a lot of local minima;
optimizing algorithms using only first-order gradient is likely to be stuck at a poor local minimum.
Secondly, exploding and vanishing gradient makes back-propagation difficult to train models with many layers stacked together,
such as recurrent neural networks and stacked Restricted Boltzmann Machines.
Even if we can tolerate the long training time and carefully deal with gradient exploding and vanishing,
the trained model is usually over-fitted to the training dataset, and not able to generalize well to the testing or future dataset.

The emergence of many novel learning algorithms and training techniques makes training deep
neural network and achieving good suboptimal minimum possible.
For example, stochastic gradient descent (SGD) with mini-batches can greatly increase the training speed comparing
to normal gradient descent on the entire dataset.
In each step of gradient descent, researchers have shown that momentum~\cite{Momentum} can
prevent SGD from oscillating across but pushing along the shallow ravine.
Along with decaying learning rate, momentum-based optimization algorithms, for example Adam~\cite{Adam},
usually help us find better local minimum.
To prevent over-fitting, researchers have proposed dropout~\cite{Dropout} to average over
exponential number of neural networks.
These learning algorithms and training techniques will directly help neural networks achieve
better performance for the network intrusion detection problem,
since they are general to any types of neural network models.


\subsection{Unsupervised Generative Models}
Another breakthrough in the deep learning area is that researchers have successfully trained
a number of unsupervised generative models that attracted much attentions.
Different from supervised models (or discriminative models) that tries to discover
the relationship between input variables and target label (or the conditional probability distribution of the targets given the input),
these models aims to learn the joint probability distribution, or joint conditional distribution,
of \textbf{all} variables for a phenomenon from the given dataset.
The resulting generative model is powerful in many ways.
First, given the well trained probability distribution, the model can synthesize meaningful data
comparable to real examples in training set.
For example, Auxiliary-Classifier Generative Adversarial Nets (AC-GAN)~\cite{AC-GAN} can generate very high quality
images after training on ImageNet dataset~\cite{ImageNet};
both stacked denoising autoencoder~\cite{DenoiseAE} and deep brief nets~\cite{DeepBeliefNets} can synthesize
handwritten digits after learning from the MNIST dataset.
Besides, the ability to generate meaningful and high quality faked data actually means that
the model have learned better feature representations from the unlabeled data itself.
As an example, it is shown that the features extracted from the hidden units of sparse autoencoder
can significantly improve the performance of support vector classifier~\cite{SparseAE}.
At last, researchers have shown that it is usually a good strategy to initialize deep neural networks
with the weights from a successfully trained generative model~\cite{DeepBeliefNets, Momentum}.

In the area of network security, the amount of network traffic data is enormously large,
usually in the order of terabytes per day in a large monitored network.
In practice, the amount of data is impossible for a human security analyst or
a group of them to review, e.g., to find patterns and label anomalies.
This situation makes an unsupervised generative model a promisingly good solution
to traffic classification since it can be trained unsupervised:
\begin{itemize}
    \item It utilizes the large amount of unlabeled data to learning useful and hierarchical features
        from the traffic data itself;
    \item It is equivalently a good way to initialize the weights of the hidden layers
        in a deep neural network, which can be further fine-tuned to be a high performance classifier.
\end{itemize}
In this project we investigate and try three types of generative models:
restricted Boltzmann machine, autoencoders and generative adversarial nets.
See section~\ref{Sec:Architectures} for details.

\subsection{Datasets}
Apart from the sophisticated and efficient learning algorithms,
abundant open data in the domain of image classification, natural language processing, machine translation, etc.
actually drives the novel and complex neural network models and make them successful.

The most vivid example would be how ImageNet dataset~\cite{ImageNet} pushed a series of deep learning models,
such as AlexNet, GoolgNet and ResNet, who greatly improved the performance of visual object recognition.
ImageNet organizes a large amount of web images by synonym set, multiple words or word phrases
describing a meaningful concept.
On average each synonym set is illustrated by 1000 quality-controlled and human-annotated images.
This project was first presented at 2009 Conference on Computer Vision and Pattern Recognition by researchers
from the CS department at Princeton University, and ran as an annual software contest known as
ImageNet Large Scale Visual Recognition Challenge (ILSVRC) since 2010.
The state-of-the-art error rate of this competition was near 25\%,
until in the year of 2012, a deep convolutional neural networks AlexNet~\cite{AlexNet} trained on GPU achieved a winning top-5 error rate of 15.3\%.
From then on, deep neural networks with convolutional blocks start its showtime.
GooLeNet~\cite{GoogLeNet} won the ILSVRC 2014 with top-5 error rate of 6.7\%.
It has 22 layers and strayed from the simple design of stacking convolutional and pooling layers.
One year later, residual block based ResNet~\cite{ResNet} pushed the error rate further down to 3.6\% with even deeper architecture.

\begin{table}[]
\centering
\caption{Popular Datasets used in Deep Learning v.s. Available Network Intrusion Detection Datasets}
\label{Tab:Datasets}
\begin{tabular}{c|c|r|r}
\multicolumn{1}{c|}{Domain}                          & Dataset       & Training Examples & Feature Dimension \\
\hline
\hline
\multirow{6}{*}{Image}                               & MNIST         & 60,000        & 784     \\
                                                     & SVHN          & 600,000       & 3072    \\
                                                     & CIFAR-10      & 60,000        & 3072    \\
                                                     & Tiny          & 80 million    & 3072    \\
                                                     & ImageNet      & 1.2 million   & 196,608 \\
\hline
\multicolumn{1}{c|}{\multirow{2}{*}{IDS}}            & UNSW-NB15     & 175,341       & 42      \\
\multicolumn{1}{l|}{}                                & NSL-KDD       & 125,973       & 41      
\end{tabular}
\end{table}

%\begin{table*}[]
%\centering
%\caption{Popular Datasets used in Deep Learning v.s. Available Network Traffic Datasets}
%\label{Tab:Datasets}
%\begin{tabular}{c|c|c|c|c}
%\multicolumn{1}{c|}{Domain}                          & Dataset Name  & \#Examples in Training Set & Feature Dimension            & Instance-Feature Ratio \\
%\hline
%\hline
%\multirow{6}{*}{Image}                               & MNIST         & 60,000        & 784 (28$\times$28 gray images)            & 76.53    \\
                                                     %& SVHN          & 600,000       & 3072 (32$\times$32 color images)          & 195.31   \\
                                                     %& CIFAR-10      & 60,000        & 3072 (32$\times$32 color images)          & 19.53    \\
                                                     %& Tiny          & 80 million    & 3072 (32$\times$32 color image            & 26041.67 \\
                                                     %& ImageNet      & 1.2 million   & 196,608 (256$\times$256 color images)     & 18.31   \\
%\hline
%\multicolumn{1}{l|}{\multirow{2}{*}{Network Traffic}} & UNSW-NB15    & 175,341       & 42                                        & 4174.79 \\
%\multicolumn{1}{l|}{}                                 & NSL-KDD      & 125,973       & 41                                        & 3072.51
%\end{tabular}
%\end{table*}
